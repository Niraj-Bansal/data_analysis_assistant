# Import all necessary libraries

from dotenv import load_dotenv, find_dotenv
import streamlit as st
import os
import pandas as pd
import langchain
from langchain.agents import load_tools, initialize_agent, Tool, AgentType
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_experimental.tools import PythonAstREPLTool
from langchain_core.output_parsers.openai_tools import JsonOutputKeyToolsParser
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
from matplotlib import pyplot as plt
from langchain.cache import InMemoryCache
from langchain_core.globals import set_llm_cache


# Defining the base LLM model, and passing the OPENAI_API_KEY
load_dotenv()
llm = ChatOpenAI()

# Allowing for In-Memory caching of Input Queries 
langchain.llm_cache = InMemoryCache()

# Add title to the chat-screen
st.title('Data Analyst Assistant')

# Define an upload button for mutliple CSV files 
uploaded_csv_list = st.file_uploader("Upload CSV files for analyzing", type = ['csv'], accept_multiple_files = True)


local_dict = {}        
if uploaded_csv_list is not None:
    for idx, df in enumerate(uploaded_csv_list):
        local_dict['df_' + str(idx)] = pd.read_csv(df)
        st.write('Sample view of {csv_name}'.format(csv_name = df.name))    
        st.write(local_dict['df_' + str(idx)].head())                                   # Printing the sample-head of each table for user confirmation

    # Defining a langchain tool to execute Python code within a REPL
    tool = PythonAstREPLTool(locals=local_dict)
    # Bining the LLM with REPL Tool
    llm_with_tools = llm.bind_tools([tool], tool_choice = tool.name)
    # Define a parser to read the "pandas query" generated by the LLM
    parser = JsonOutputKeyToolsParser(key_name=tool.name, first_tool_only=True)

    # Define a function to extract chat history for a tool call
    # Used to generate conversational output from the LLM
    def _get_chat_history(x: dict) -> list:
        """Parse the chain output up to this point into a list of chat history messages to insert in the prompt."""
        ai_msg = x["ai_msg"]
        tool_call_id = x["ai_msg"].additional_kwargs["tool_calls"][0]["id"]
        tool_msg = ToolMessage(tool_call_id=tool_call_id, content=str(x["tool_output"]))
        return [ai_msg, tool_msg]

    # Define the heads and schemas for all input csv files
    df_template = """\`\`\`python
                    {df_name}.head().to_markdown()
                    >>> {df_head}
                    \`\`\`"""
    df_context = "\n\n".join(
            df_template.format(df_head=_df.head().to_markdown(), df_name=df_name)
            for df_name, _df in local_dict.items())

    # Define a system prompt for the model to generate Pandas code-snippets
    system = f"""You have access to a number of pandas dataframes. \
            Here is a sample of rows from each dataframe and the python code that was used to generate the sample:

            {df_context}

            Given a user question about the dataframes, write the Python code to answer it. \
            Don't assume you have access to any libraries other than built-in Python ones, pandas and matplotlib. \
            Make sure to refer only to the variables mentioned above."""

    # A dynamic prompt which adds user-question as HumanMessage
    # Also contains MessagesPlaceholder to allow for dynamic chat_history at execution
    prompt = ChatPromptTemplate.from_messages([("system", system), 
                ("human", "{question}"), MessagesPlaceholder("chat_history", optional=True)])

    # Define a chain with Passthrough-Runnable, which will output a dictionary contains keys: - 
    # ai_msg, tool_output, chat_history, response. The "response" one is used by us 
    chain = (
        RunnablePassthrough.assign(ai_msg=prompt | llm_with_tools)          # Generates the pandas-query based on user question 
        .assign(tool_output=itemgetter("ai_msg") | parser | tool)           # Passes the pandas query through REPL tool for execution
        .assign(chat_history=_get_chat_history)                             # Extract the chat_history containing tool-call info 
        .assign(response=prompt | llm | StrOutputParser())                  # Re-send the prompt with chat_history to extract conversational-output
    )

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
        
    
    # Display chat messages from history on app re-run
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Accept user question
    question = st.chat_input("Enter your question here")
    if question:
        # Add user question in messages of session_state
        with st.chat_message("user"):
            st.markdown(question)
        st.session_state.messages.append({'role' : "user", "content" : question})

        # Generate model-output, and parse its 'response' key from output dict
        result = chain.invoke({'question' : question})['response']
        
        # Add bot-answer in messages of session_state
        with st.chat_message('assistant'):
            st.markdown(result)
        st.session_state.messages.append({'role' : "assistant", "content" : result})
